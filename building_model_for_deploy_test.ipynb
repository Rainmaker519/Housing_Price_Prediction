{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcdcbadc",
   "metadata": {},
   "source": [
    "# Predicting Rent Prices Based on Unit/House Data\n",
    "*Exploratory Data Analysis, Data Processing, Model Development, and Model Deployment by Charles Selden*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e10c97",
   "metadata": {},
   "source": [
    "## Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57164ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mticker  \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from pandas.api.types import CategoricalDtype\n",
    "from sklearn.preprocessing import scale \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "from scipy.stats import chi2_contingency\n",
    "import researchpy as rp\n",
    "import math\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad9b7e6",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416c377",
   "metadata": {},
   "source": [
    "## Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f7988ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Charlie/Documents/data/house_data/House_Rent_Dataset.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2852\\2406595333.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"~/Documents/data/house_data/House_Rent_Dataset.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 811\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    812\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1038\u001b[0m             )\n\u001b[0;32m   1039\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1040\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1042\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m         \"\"\"\n\u001b[1;32m--> 222\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m    223\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    700\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    701\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    703\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Charlie/Documents/data/house_data/House_Rent_Dataset.csv'"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"~/Documents/data/house_data/House_Rent_Dataset.csv\")\n",
    "columns = list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452488f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_description = {}\n",
    "\n",
    "descriptions_temp = [\"Posted On: The date on which the house listing was posted.\",\n",
    "\"BHK: Number of Bedrooms, Hall, Kitchen.\",\n",
    "\"Rent: Rent of the Houses/Apartments/Flats.\",\n",
    "\"Size: Size of the Houses/Apartments/Flats in Square Feet.\",\n",
    "\"Floor: Houses/Apartments/Flats situated in which Floor and Total Number of Floors (Example: Ground out of 2, 3 out of 5, etc.)\",\n",
    "\"Area Type: Size of the Houses/Apartments/Flats calculated on either Super Area or Carpet Area or Build Area.\",\n",
    "\"Area Locality: Locality of the Houses/Apartments/Flats.\",\n",
    "\"City: City where the Houses/Apartments/Flats are Located.\",\n",
    "\"Furnishing Status: Furnishing Status of the Houses/Apartments/Flats, either it is Furnished or Semi-Furnished or Unfurnished.\",\n",
    "\"Tenant Preferred: Type of Tenant Preferred by the Owner or Agent.\",\n",
    "\"Bathroom: Number of Bathrooms.\",\n",
    "\"Point of Contact: Whom should you contact for more information regarding the Houses/Apartments/Flats.\"]\n",
    "\n",
    "for i in range(len(list(data.columns))):\n",
    "    column_to_description[columns[i]] = descriptions_temp[i]\n",
    "                                  \n",
    "for i in columns:\n",
    "    print(column_to_description[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd625f",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d64efe8",
   "metadata": {},
   "source": [
    "# Exploring the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6acec8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55774d0c",
   "metadata": {},
   "source": [
    "## Posted On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb09fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"Posted On\"],\"\\n\")\n",
    "print(data[\"Posted On\"],\"\\n\")\n",
    "posted_counts = data[\"Posted On\"].value_counts()\n",
    "print(posted_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4e4a94",
   "metadata": {},
   "source": [
    "#### Maybe convert dates to int representing time since first listed date/start of 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88015638",
   "metadata": {},
   "source": [
    "## BHK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa73447",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"BHK\"],\"\\n\")\n",
    "print(data[\"BHK\"],\"\\n\")\n",
    "print(data[\"BHK\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f2ccf2",
   "metadata": {},
   "source": [
    "#### Simple, works in current form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf841d31",
   "metadata": {},
   "source": [
    "## Rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83028a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"Rent\"],\"\\n\")\n",
    "print(data[\"Rent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540587be",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[\"Size\"],np.sort(data[\"Rent\"]))\n",
    "plt.gca().yaxis.set_major_formatter(mticker.FormatStrFormatter('%.0f Rupees'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6e163e",
   "metadata": {},
   "source": [
    "*Lets put a limit on the y axis to ignore the extreme outliers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dde7ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[\"Size\"],np.sort(data[\"Rent\"]))\n",
    "plt.gca().yaxis.set_major_formatter(mticker.FormatStrFormatter('%.0f Rupees'))\n",
    "plt.ylim(0,600000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d327a",
   "metadata": {},
   "source": [
    "*As a scalar value, it's easy to use. Our y value, what we want to predict using the model. A few extremely high outliers, but the majority values are under 100,000 and an even larger majority are under 400,000*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8ae035",
   "metadata": {},
   "source": [
    "## Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75afdc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"Size\"],\"\\n\")\n",
    "print(data[\"Size\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e6f0aa",
   "metadata": {},
   "source": [
    "*In square feet, makes using much easier than a categorical version.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efc24a3",
   "metadata": {},
   "source": [
    "## Floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96680ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"Floor\"],\"\\n\")\n",
    "print(data[\"Floor\"],\"\\n\")\n",
    "print(data[\"Floor\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d16f37",
   "metadata": {},
   "source": [
    "*Actual values are scalar but in a string format, split into two seperate columns for the floor the room is on as well as the total number of floors the place has.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ac1a3",
   "metadata": {},
   "source": [
    "## Area Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787a98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"Area Type\"],\"\\n\")\n",
    "print(data[\"Area Type\"],\"\\n\")\n",
    "print(data[\"Area Type\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06b1871",
   "metadata": {},
   "source": [
    "*Super area includes sq feet for areas the tenant will have access to outside his apartment/house (stairways, public areas, hallways), while carpet area is just the apartment or house itself.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022d20d3",
   "metadata": {},
   "source": [
    "## Area Locality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add89193",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"Area Locality\"],\"\\n\")\n",
    "print(data[\"Area Locality\"],\"\\n\")\n",
    "print(data[\"Area Locality\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66edf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data[\"Area Locality\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd505290",
   "metadata": {},
   "source": [
    "*Too many categorical variables with only a few data points to one-hot encode or label encode, just use city instead.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645489c7",
   "metadata": {},
   "source": [
    "## City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5419d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"City\"],\"\\n\")\n",
    "print(data[\"City\"],\"\\n\")\n",
    "print(data[\"City\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd70086a",
   "metadata": {},
   "source": [
    "*To be used instead of Locality, the issue of how to encode this categorical data is still a concern. Forgot that one-hot encoding created a column for each value which have binary values, this definitely is the best encoding scheme to use here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9b932",
   "metadata": {},
   "source": [
    "## Furnishing Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989b781f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"Furnishing Status\"],\"\\n\")\n",
    "print(data[\"Furnishing Status\"],\"\\n\")\n",
    "print(data[\"Furnishing Status\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472a033e",
   "metadata": {},
   "source": [
    "*Categorical Data but seems like it would likely perform well if label-encoded as the categories are a scale from Unfurnished  through Furnished. Just need to make sure that unfurnished = 0, semi-furnished = 1, and furnished = 2.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9d0895",
   "metadata": {},
   "source": [
    "## Tenant Preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deaf09fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"Tenant Preferred\"],\"\\n\")\n",
    "print(data[\"Tenant Preferred\"],\"\\n\")\n",
    "print(data[\"Tenant Preferred\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767e3d26",
   "metadata": {},
   "source": [
    "*Potentially able to use one-hot encoding for this, but might not give as much information as it potentially could.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8e5469",
   "metadata": {},
   "source": [
    "## Bathroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49dec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"Bathroom\"],\"\\n\")\n",
    "print(data[\"Bathroom\"],\"\\n\")\n",
    "print(data[\"Bathroom\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18ac64",
   "metadata": {},
   "source": [
    "*No problem to use, simple scalar integer value.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977949a5",
   "metadata": {},
   "source": [
    "## Point of Contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6494ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_to_description[\"Point of Contact\"],\"\\n\")\n",
    "print(data[\"Point of Contact\"], \"\\n\")\n",
    "print(data[\"Point of Contact\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71285ff1",
   "metadata": {},
   "source": [
    "*Maybe remove contact builder since with just one data point we can't learn how this factors into the regression robustly.*\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9fa5f9",
   "metadata": {},
   "source": [
    "# Checking for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26afa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in columns:\n",
    "    print(str(i) + \":\",data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86aa3349",
   "metadata": {},
   "source": [
    "*There are no missing values in this dataset.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826bbc89",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fc766b",
   "metadata": {},
   "source": [
    "# Data Engineering and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdeb0d8",
   "metadata": {},
   "source": [
    "## Steps to be taken based on EDA:\n",
    "1. `Posted On`: Convert Date to time since start of 2022.\n",
    "2. `Size`: Maybe scale this using *Area Type* if it doesn't work well in the regression elsewise. For now having *Area Type* as a label-encoded variable which learns a flat value to adjust the outcome by might be fine, but the difference between different *Size*s will vary as some super-areas will severely overestimate *Size* and some will only minimally overestimate *Size*.\n",
    "3. `Floor`: One of two options seems optimal. Both involve splitting the active floor and the total floors given the building first. The first option is just having both of those as seperate columns, the second option involves the floor the unit is on as the first column, and the ratio of that to the total floors in the building as the second.\n",
    "The first option seems like the data is presented more clearly, but the second allows the relationship between the two to be passed directly in, as learning that might be suboptimal in terms of limiting learning other relationships. Call these columns *On Floor* and *Building Floors* in either case.\n",
    "4. `Area Type`: If used to scale *Size* then don't use this column, otherwise use binary one-hot encoding for *Carpet Area* and *Super Area*, and throw out *Built Area* as that only has two data points.\n",
    "5. `City`: This needs to be one-hot encoded, definitely good in this case as there are only a small number of cities with decent data population for each. \n",
    "6. `Furnishing Status`: Label encode with the following system. *Unfurnished* = 0, *Semi-Furnished* = 1, *Furnished* = 2.\n",
    "7. `Tenant Preferred`: Do a kind of custom one-hot encoding. Normally each value would get its own column, but here we can just make two instead of three as one just represents both of the previous options simultaneously.\n",
    "8. `Point of Contact`: Just drop the *Contact Builder* as it only has a single data point, the other two can be label encoded with 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ce215a",
   "metadata": {},
   "source": [
    "***\n",
    "## Taking these Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4389228",
   "metadata": {},
   "source": [
    "### Initial Data Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a0a5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b62c9",
   "metadata": {},
   "source": [
    "### Step One - Making a Function to Update Posted On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5848e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "posted_on_temp = data[\"Posted On\"]\n",
    "posted_on_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6351454e",
   "metadata": {},
   "source": [
    "*First we can check whether all the dates start in 2022, this same method can be applied using whatever year is the earliest date in the dataset.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b69a9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dates = []\n",
    "\n",
    "for i in range(len(posted_on_temp)):\n",
    "    split_dates.append(posted_on_temp[i].split(\"-\"))\n",
    "    \n",
    "    if split_dates[i][0] != \"2022\":\n",
    "        print(\"diff date\",split_dates[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3c4205",
   "metadata": {},
   "source": [
    "*Since all the data is from 2022, we will set 2022-01-01 to be 0, and every day past will increment.*\n",
    "\n",
    "*First we need some way to tell the number of days in a month given the month and year.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd834bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberOfDays(y, m):\n",
    "      leap = 0\n",
    "      if y% 400 == 0:\n",
    "         leap = 1\n",
    "      elif y % 100 == 0:\n",
    "         leap = 0\n",
    "      elif y% 4 == 0:\n",
    "         leap = 1\n",
    "      if m==2:\n",
    "         return 28 + leap\n",
    "      list = [1,3,5,7,8,10,12]\n",
    "      if m in list:\n",
    "         return 31\n",
    "      return 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10290395",
   "metadata": {},
   "source": [
    "*Now we can build our update function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44e58d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updatePostedOnToScalar(data):\n",
    "    list_of_date_num_equivalent = []    \n",
    "    \n",
    "    for i in range(len(split_dates)):\n",
    "        date_num_equivalent = 0\n",
    "        \n",
    "        for j in range(int(split_dates[i][1])):\n",
    "            date_num_equivalent = date_num_equivalent + numberOfDays(int(split_dates[i][0]),j)\n",
    "            \n",
    "        date_num_equivalent = date_num_equivalent + int(split_dates[i][2])\n",
    "        \n",
    "        list_of_date_num_equivalent.append(date_num_equivalent)\n",
    "        \n",
    "    list_of_date_num_equivalent = np.array(list_of_date_num_equivalent)\n",
    "        \n",
    "    data[\"Posted On\"] = list_of_date_num_equivalent\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a29780",
   "metadata": {},
   "source": [
    "### Step Two - Making a Function to Adjust Size if Necessary\n",
    "*Nothing necessary here so far.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d67282",
   "metadata": {},
   "source": [
    "### Step Three - Making a Function to Split Floor Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172c1921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitFloorIntoTwo(data):\n",
    "    original_floor = data[\"Floor\"]\n",
    "    floor_on = []\n",
    "    floor_out_of = []\n",
    "    for i in range(len(original_floor)):\n",
    "        split = original_floor[i].split(\" out of \")\n",
    "        if len(split) == 1:\n",
    "            if split[0] == 'Ground':\n",
    "                floor_on.append(0)\n",
    "                floor_out_of.append(0)\n",
    "            else:\n",
    "                floor_on.append(split[0])\n",
    "                floor_out_of.append(split[0])\n",
    "        else:  \n",
    "            #Ground is 0, others are basement and such so all other non-int convertables go to -1\n",
    "            if split[0] == 'Ground':\n",
    "                floor_on.append(0)\n",
    "            else:\n",
    "                try:\n",
    "                    floor_on.append(int(split[0]))\n",
    "                except:\n",
    "                    floor_on.append(-1)\n",
    "            floor_out_of.append(int(split[1]))\n",
    "    \n",
    "    floor_on = np.array(floor_on)\n",
    "    floor_out_of = np.array(floor_out_of)\n",
    "    \n",
    "    data[\"Floor On\"] = floor_on\n",
    "    data[\"Floor Out Of\"] = floor_out_of\n",
    "\n",
    "    data.drop(\"Floor\",axis=1,inplace=True)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2d1b39",
   "metadata": {},
   "source": [
    "### Step Four - Making a Function to Label Encode a Given Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ce6ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labelEncodeColumn(data,column_name):\n",
    "    if column_name == \"Furnishing Status\":\n",
    "        cat_type = CategoricalDtype(categories=[\"Unfurnished\", \"Semi-Furnished\", \"Furnished\"], ordered=True)\n",
    "        data[column_name] = data[column_name].astype(cat_type)\n",
    "        data[column_name] = data[column_name].cat.codes\n",
    "    else:\n",
    "        data[column_name] = data[column_name].astype('category')\n",
    "        data[column_name] = data[column_name].cat.codes\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d83df4",
   "metadata": {},
   "source": [
    "### Step Five - Making a Function to One-Hot Encode a Given Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5e27ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneHotEncodeColumn(data,column_name):\n",
    "    encoder = OneHotEncoder(handle_unknown='ignore',sparse=False)\n",
    "    if column_name == \"Tenant Preferred\":\n",
    "        bachelors = []\n",
    "        family = []\n",
    "        for i in range(len(data[\"Tenant Preferred\"])):\n",
    "            if data[\"Tenant Preferred\"][i] == \"Bachelors\":\n",
    "                bachelors.append(1)\n",
    "                family.append(0)\n",
    "            elif data[\"Tenant Preferred\"][i] == \"Family\":\n",
    "                bachelors.append(0)\n",
    "                family.append(1)\n",
    "            elif data[\"Tenant Preferred\"][i] == \"Bachelors/Family\":\n",
    "                bachelors.append(1)\n",
    "                family.append(1)\n",
    "            else:\n",
    "                print(\"issue w tenant preferred encoding\")\n",
    "        bachelors = pd.Series(bachelors)\n",
    "        bachelors.name = \"Bachelors\"\n",
    "        family = pd.Series(family)\n",
    "        family.name = \"Family\"\n",
    "\n",
    "        data = pd.concat([data,bachelors,family],axis=1)\n",
    "\n",
    "        return data\n",
    "\n",
    "    else:\n",
    "        encoded_data = pd.DataFrame(encoder.fit_transform(data[[column_name]]))\n",
    "\n",
    "        feature_names = encoder.get_feature_names_out()\n",
    "        for i in range(len(feature_names)):\n",
    "            feature_names[i] = feature_names[i].split(\"_\")\n",
    "        for i in range(len(feature_names)):\n",
    "            feature_names[i] = feature_names[i][1]\n",
    "\n",
    "        encoded_data.columns = feature_names\n",
    "\n",
    "        data = pd.concat([data,encoded_data],axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e197a97d",
   "metadata": {},
   "source": [
    "***\n",
    "## Processing the Data for Training our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18420fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b7510",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1509989e",
   "metadata": {},
   "source": [
    "## Step One - Update Posted On to Scalar Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53274645",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Posted On\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = updatePostedOnToScalar(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690e38a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Posted On\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40027d01",
   "metadata": {},
   "source": [
    "*Check that the update didn't create any null values unintentionally.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5088044",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "for i in columns:\n",
    "    print(str(i) + \":\",data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f932ba6d",
   "metadata": {},
   "source": [
    "### Step Two - Adjust Size *(if necessary)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2e4260",
   "metadata": {},
   "source": [
    "### Step Three - Split Floor Column into Floor On and Floor Out Of"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651157f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Floor\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a7a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = splitFloorIntoTwo(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60afda44",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Floor On\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c3f6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Floor Out Of\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f95c1",
   "metadata": {},
   "source": [
    "*Check that the update didn't create any null values unintentionally.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64373c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "for i in columns:\n",
    "    print(str(i) + \":\",data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbbb825",
   "metadata": {},
   "source": [
    "### Step Four - Encode Area Type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b239cc",
   "metadata": {},
   "source": [
    "*First we need to drop Built Area as it has too few data points.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9cde8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Area Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5463c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[data['Area Type'] == 'Built Area'].index)\n",
    "data['Area Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539f001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8e239",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15679aed",
   "metadata": {},
   "source": [
    "*Check that dropping rows didn't create any null values unintentionally.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0958adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "for i in columns:\n",
    "    print(str(i) + \":\",data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb28a41",
   "metadata": {},
   "source": [
    "*We can label encode into two categories after having dropped Built Area. Since there are only two potential categories, we don't need to worry about ordinality of the categories.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966cdd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Area Type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c98f741",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = labelEncodeColumn(data,'Area Type')\n",
    "data['Area Type']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c227d075",
   "metadata": {},
   "source": [
    "*Check that the update didn't create any null values unintentionally.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb454e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "for i in columns:\n",
    "    print(str(i) + \":\",data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9889bc19",
   "metadata": {},
   "source": [
    "### Step Five - One-Hot Encoding for City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee43484",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['City']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8fa208",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = oneHotEncodeColumn(data,'City')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536824ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"City\",axis=1)\n",
    "data = data.drop(\"Area Locality\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f0ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d11294",
   "metadata": {},
   "source": [
    "*Check that the update didn't create any null values unintentionally.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00741dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "for i in columns:\n",
    "    print(str(i) + \":\",data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3a7261",
   "metadata": {},
   "source": [
    "### Step Six - Label *(or maybe ordinal)* Encode Furnishing Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa491ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Furnishing Status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc114560",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = labelEncodeColumn(data,'Furnishing Status')\n",
    "data['Furnishing Status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cf66f",
   "metadata": {},
   "source": [
    "*Check that the update didn't create any null values unintentionally.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1f9b7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "for i in columns:\n",
    "    print(str(i) + \":\",data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c21dc5",
   "metadata": {},
   "source": [
    "### Step Seven - One-Hot Encode Tenant Preferred *(with bachelor and family both binary so they can include bachelor/family with just those columns)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25d4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Tenant Preferred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387fa106",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = oneHotEncodeColumn(data,\"Tenant Preferred\")\n",
    "columns = data.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfa03e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"Tenant Preferred\",axis=1)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95971df2",
   "metadata": {},
   "source": [
    "*Check that the update didn't create any null values unintentionally.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e61ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "for i in columns:\n",
    "    print(str(i) + \":\",data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdfafae",
   "metadata": {},
   "source": [
    "### Step Eight - Label Encode Point of Contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb46735",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Point of Contact']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90acab4",
   "metadata": {},
   "source": [
    "*First I need to drop Contact Builder given the scarcity of data using that value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac75c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(data[data['Point of Contact'] == 'Contact Builder'].index)\n",
    "data['Point of Contact'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69366f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index()\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ee1fa",
   "metadata": {},
   "source": [
    "*Check that dropping rows didn't create any null values unintentionally.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f36fbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "for i in columns:\n",
    "    print(str(i) + \":\",data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b360da9",
   "metadata": {},
   "source": [
    "*Next I just label encode the Point of Contact column.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = labelEncodeColumn(data,'Point of Contact')\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64587e02",
   "metadata": {},
   "source": [
    "*Now after we remove the columns representing the indices at points in the process, we are finished processing our data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"level_0\",axis=1)\n",
    "data = data.drop(\"index\",axis=1)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaf694d",
   "metadata": {},
   "source": [
    "*Now we just need to make sure that all of our data types are numbers (just not strings)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b419e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "for i in columns:\n",
    "    data[i] = data[i].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4614e191",
   "metadata": {},
   "source": [
    "*Check that the update didn't create any null values unintentionally.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e48471",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "for i in columns:\n",
    "    print(str(i) + \":\",data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04fbdeb",
   "metadata": {},
   "source": [
    "# Testing for Linear Relationships in Our Variables with Rent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cea6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_linear_relationship(compare_from):\n",
    "    compare_col = compare_from\n",
    "    theta = np.polyfit(data[compare_col], data['Rent'],1)\n",
    "    y_line = theta[1] + theta[0] * data[compare_col]\n",
    "    plt.scatter(data[compare_col], data['Rent'], color='red')\n",
    "    plt.plot(data[compare_col], y_line, 'b')\n",
    "    plt.title('Rent Vs ' + str(compare_col), fontsize=14)\n",
    "    plt.xlabel(compare_col, fontsize=14)\n",
    "    plt.ylabel('Rent', fontsize=14)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    print(\"The slope of the best fit for\", compare_col, \"is \" + str(theta[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b6ed4e",
   "metadata": {},
   "source": [
    "### Posted On"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f91a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_linear_relationship(\"Posted On\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97037158",
   "metadata": {},
   "source": [
    "### BHK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_linear_relationship(\"BHK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea63a1e",
   "metadata": {},
   "source": [
    "### Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e0f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_linear_relationship(\"Size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3196359c",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_linear_relationship(\"Area Type\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e7837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_linear_relationship(\"Furnishing Status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98510c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_linear_relationship(\"Bathroom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_linear_relationship(\"Point of Contact\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f869c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_linear_relationship(\"Floor On\")\n",
    "check_linear_relationship(\"Floor Out Of\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd939e",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_linear_relationship(\"Bachelors\")\n",
    "check_linear_relationship(\"Family\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57534ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "check_linear_relationship(\"Bangalore\")\n",
    "check_linear_relationship(\"Chennai\")\n",
    "check_linear_relationship(\"Delhi\")\n",
    "check_linear_relationship(\"Hyderabad\")\n",
    "check_linear_relationship(\"Kolkata\")\n",
    "check_linear_relationship(\"Mumbai\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bff35c",
   "metadata": {},
   "source": [
    "### We know now that despite not directly appearing linear, each of our variables has an acceptably strong linear relationship. While the clustered data points make these difficult to see, the best fit lines' slopes indicate the actual trend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fcda86",
   "metadata": {},
   "source": [
    "### Along with accomplishing initial testing for a basic linear regression model, we can perform PCA to determine if we are using too much information along with get a baseline score for validation purposes given each model we want to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef670c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(data,reg_type=\"linear_regression\",repeats=3):\n",
    "    columns = data.columns\n",
    "    X = data[columns]\n",
    "    y = np.ravel(data[[\"Rent\"]])\n",
    "    X = X.drop(\"Rent\",axis=1)\n",
    "    num_points = len(data)\n",
    "    \n",
    "    pca = PCA()\n",
    "    X_reduced = pca.fit_transform(scale(X))\n",
    "    \n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=repeats, random_state=.33)\n",
    "    \n",
    "    if reg_type == 'linear':\n",
    "        regr = LinearRegression()\n",
    "    elif reg_type == 'random_forest':\n",
    "        regr = RandomForestRegressor()\n",
    "    elif reg_type == 'lasso':\n",
    "        regr = Lasso()\n",
    "    else:\n",
    "        print(\"you need a valid regression type for pca\")\n",
    "        return\n",
    "    mse = []\n",
    "    \n",
    "    score = -1*model_selection.cross_val_score(regr,\n",
    "           np.ones((len(X_reduced),1)), y, cv=cv,\n",
    "           scoring='neg_mean_squared_error').mean()  \n",
    "    mse.append(score/num_points)\n",
    "    \n",
    "    for i in np.arange(1, len(data.columns)):\n",
    "        score = -1*model_selection.cross_val_score(regr,\n",
    "               X_reduced[:,:i], y, cv=cv, scoring='neg_mean_squared_error').mean()\n",
    "        mse.append(score/num_points)\n",
    "        \n",
    "    plt.plot(mse)\n",
    "    plt.xlabel('Number of Used Components')\n",
    "    plt.ylabel('MSE / Sample Size')\n",
    "    plt.title('Rent')\n",
    "    display(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0ffa28",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = data.columns\n",
    "X = data[columns]\n",
    "y = data[[\"Rent\"]]\n",
    "X = X.drop(\"Rent\",axis=1)\n",
    "display(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d636e609",
   "metadata": {},
   "source": [
    "*First we scale our predictor variables between 0 and 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7172d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_reduced = pca.fit_transform(scale(X))\n",
    "X_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697d09e6",
   "metadata": {},
   "source": [
    "*Next we define the cross validation method we will use in our evaluation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b5d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857df34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = LinearRegression()\n",
    "#regr = RandomForestRegressor()\n",
    "mse = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a687ad76",
   "metadata": {},
   "source": [
    "*Next we calculate MSE with only the intercept.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea989ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = -1*model_selection.cross_val_score(regr,\n",
    "           np.ones((len(X_reduced),1)), y, cv=cv,\n",
    "           scoring='neg_mean_squared_error').mean()  \n",
    "mse.append(score/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de5a7bc",
   "metadata": {},
   "source": [
    "*Finally we calculate MSE using cross-validation, adding one component at a time.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4330f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.arange(1, len(data.columns)):\n",
    "    score = -1*model_selection.cross_val_score(regr,\n",
    "               X_reduced[:,:i], y, cv=cv, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ea799",
   "metadata": {},
   "source": [
    "*Plot cross-validation results (found using multiple linear regression with MSE)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03569420",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mse)\n",
    "plt.xlabel('Number of Used Components')\n",
    "plt.ylabel('MSE / Sample Size')\n",
    "plt.title('Rent')\n",
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b4c72",
   "metadata": {},
   "source": [
    "*We can clearly see that each of the principal components reduces the overall MSE of the model except for the last two, meaning we likely have some correlation between some of our variables. This means that next up is testing for correlation or multicollinearity, if the variables are all correlated or if each correlated variable is correlated to a different variable.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b70cab",
   "metadata": {},
   "source": [
    "## Testing Variables for Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb576abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residuals(data,skl_model,trained):\n",
    "    y = data[\"Rent\"]\n",
    "    X = data.drop(\"Rent\",axis=1)\n",
    "    X_train = X[:int(len(X)*.7)]\n",
    "    y_train = y[:int(len(X)*.7)]\n",
    "    X_test = X[int(len(X)*.7):]\n",
    "    y_test = y[int(len(X)*.7):]\n",
    "    \n",
    "    \n",
    "    X_train = X_train.reset_index()\n",
    "    X_test = X_test.reset_index()\n",
    "    y_train = y_train.reset_index()\n",
    "    y_test = y_test.reset_index()\n",
    "    \n",
    "    \n",
    "    if not trained:\n",
    "        skl_model.fit(X_train,y_train)\n",
    "    \n",
    "    full_predictions = skl_model.predict(X_test)\n",
    "    predictions = []\n",
    "    residuals = []\n",
    "    \n",
    "    y_test = y_test.drop(\"index\",axis=1)\n",
    "    \n",
    "    for i in range(len(full_predictions)):\n",
    "        predictions.append(full_predictions[i][1])\n",
    "    \n",
    "    for i in range(len(predictions)):\n",
    "        residuals.append(abs(predictions[i] - y_test[\"Rent\"][i]))\n",
    "           \n",
    "    return residuals\n",
    "                         \n",
    "resids = get_residuals(data,regr,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90949dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "durbin_watson(resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd52effe",
   "metadata": {},
   "source": [
    "**The durbin_watson statistic being under 1.5 indicates a relatively strong autocorrelation within the used variables.**\n",
    "\n",
    "This means we now need to test for which of our variables strongly correlate with each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc11781d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#chisquare(data).statistic\n",
    "\n",
    "#covariance = SUM((xi - avgi)(yj - avgj))/n\n",
    "\n",
    "def areScalarVariablesCorrelated(data,varA,varB):\n",
    "    var1 = scale(data[varA])\n",
    "    var2 = scale(data[varB])\n",
    "    \n",
    "    avg1 = var1.mean()\n",
    "    avg2 = var2.mean()\n",
    "    \n",
    "    if len(var1) != len(var2):\n",
    "        print(\"Please use variables with the same number of entries.\")\n",
    "        return None\n",
    "    \n",
    "    total_sum = 0\n",
    "    for i in range(len(var1)):\n",
    "        total_sum = total_sum + (var1[i] - avg1)*(var2[i] - avg2)\n",
    "    \n",
    "    return total_sum/len(var1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e8ac6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cba05aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scalarCorrelationMatrix(data):\n",
    "    scalar_columns = ['Posted On','BHK','Size','Bathroom','Floor On','Floor Out Of']\n",
    "\n",
    "    data_columns = data.columns\n",
    "    \n",
    "    for i in scalar_columns:\n",
    "        if not i in data.columns:\n",
    "            scalar_columns.remove(i)\n",
    "    \n",
    "    correlation_columns = []\n",
    "\n",
    "    for i in scalar_columns:\n",
    "        list_column = []\n",
    "        for j in scalar_columns:\n",
    "            list_column.append(areScalarVariablesCorrelated(data,i,j))\n",
    "        correlation_columns.append(pd.Series(list_column,name=i,dtype='float64'))\n",
    "\n",
    "    correlation_frame = pd.concat(correlation_columns,axis=1)\n",
    "    correlation_frame.index = scalar_columns\n",
    "    \n",
    "    return correlation_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94fab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scalarCorrelationMatrix(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c6ccb1",
   "metadata": {},
   "source": [
    "**Here the correlation matrix shows the level of correlation between any of the two scalar variables.**\n",
    "\n",
    "The strongest correlations are as follows:\n",
    "\n",
    "1. Floor on and Floor Out Of - This correlation is pretty simple, Floor Out Of is the maximum value that Floor On could be, meaning that as Floor Out Of increases, the potential size of Floor On does as well.\n",
    "\n",
    "    To fix this we could make Floor On into a ratio between current Floor On and Floor Out Of, making the Floor Out Of represent the building size while Floor On will say where in the building it is without scaling linearly along with Floor Out Of.\n",
    "\n",
    "\n",
    "2. Bathroom and BHK - This correlation makes a lot of sense as well, given that Bathroom tells the number of available bathrooms while BHK tells the number of bathroons, hallways, and kitchens. \n",
    "\n",
    "3. Size and Bathroom - The correlation makes sense as the larger a place is, generally the more bathrooms it has and apparently that trend is strong enough to show correlation here.\n",
    "\n",
    "4. Size and BHK - This is part of the same multicollinear issue with the previous two. \n",
    "\n",
    "    To try to fix all three of these we can try this:\n",
    "\n",
    "    First, scale BHK and Bathroom by Size (BHK/Size,Bathroom/Size).\n",
    "\n",
    "    Next, update BHK to be itself minus Bathroom.\n",
    "\n",
    "Check for collinearity after this, should remove correlation with Size for Bathroom and BHK, but still need to see if the processing will affect how the model reads the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5440fd75",
   "metadata": {},
   "source": [
    "## Floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2734742d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleFloor(data):\n",
    "    data[\"Floor On\"].astype(\"float32\",copy=False)\n",
    "    floor_on = data[\"Floor On\"]\n",
    "    floor_out_of = data[\"Floor Out Of\"]\n",
    "    for i in range(len(floor_on)):\n",
    "        if floor_on[i] == 0:\n",
    "            floor_on[i] = 0\n",
    "        else:\n",
    "            floor_on[i] = floor_on[i] / floor_out_of[i]\n",
    "    data[\"Floor On\"] = floor_on\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fc617b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f138ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scaleFloor(data)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0687fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scalarCorrelationMatrix(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5076f254",
   "metadata": {},
   "source": [
    "**Dropped from .86 to .084, successfully removed significant correlation from Floor On and Floor Out Of.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46ec259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sizeBathroomBHKScale(data):\n",
    "    #First scale Bathroom and BHK by Size\n",
    "    data[\"Bathroom\"].astype(\"float32\",copy=False)\n",
    "    data[\"BHK\"].astype(\"float32\",copy=False)\n",
    "    \n",
    "    bathroom = data[\"Bathroom\"]\n",
    "    bhk = data[\"BHK\"]\n",
    "    size = data[\"Size\"]\n",
    "    \n",
    "    for i in range(len(bathroom)):\n",
    "        if size[i] == 0:\n",
    "            size[i] = 1\n",
    "        bathroom[i] = bathroom[i] + bhk[i]\n",
    "        bathroom[i] = bathroom[i] / size[i]\n",
    "            \n",
    "    data[\"Bathroom\"] = bathroom\n",
    "    data = data.drop(\"BHK\",axis=1)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d8a59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sizeBathroomBHKScale(data)\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scalarCorrelationMatrix(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb1ee4c",
   "metadata": {},
   "source": [
    "**Now some of our correlation is negative, but the degree is significantly smaller indicating much less correlation between the three variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47bd937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resids = get_residuals(data,LinearRegression(),False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a22f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "durbin_watson(resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b835fd",
   "metadata": {},
   "source": [
    "**We've improved our durbin watson statistic from .95 to 1.19, however our goal is still 1.5.**\n",
    "\n",
    "We have looked close at our scalar variables, now it's time to check our categorical vars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e767ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = [\"Area Type\",\"Furnishing Status\",\"Point of Contact\",\"Bachelors\",\"Family\",\n",
    "            \"Bangalore\",\"Chennai\",\"Delhi\",\"Hyderabad\",\"Kolkata\",\"Mumbai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e94413e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCatCorrelation(data):\n",
    "    result_holder = []\n",
    "    for i in cat_vars:\n",
    "        #cat_holder.append(data[i].value_counts())\n",
    "        for j in cat_vars:\n",
    "            crosstab, test_results, expected = rp.crosstab(data[i], data[j],\n",
    "                                               test= \"chi-square\",\n",
    "                                               expected_freqs= True,\n",
    "                                               prop= \"cell\")\n",
    "            result_holder.append([crosstab,test_results,expected])\n",
    "        \n",
    "    return result_holder\n",
    "    \n",
    "result = getCatCorrelation(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f06f816",
   "metadata": {},
   "source": [
    "**Phi and Cramer's V\tInterpretation**\n",
    "\n",
    "0.25:\tVery strong\n",
    "\n",
    "0.15:\tStrong\n",
    "\n",
    "0.10:\tModerate\n",
    "\n",
    "0.05:\tWeak\n",
    "\n",
    "0:\tNo or very weak\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e41dae",
   "metadata": {},
   "source": [
    "## Correlation Strength per Categorical Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b34a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def catCorrMatrix(catCorrResults):\n",
    "    corr_strength = pd.DataFrame()\n",
    "    corr_strength_constructor = []\n",
    "\n",
    "    #n is the starting point (index wise) for the current var\n",
    "    n = 0\n",
    "    interp = False\n",
    "    for i in range(len(cat_vars)): \n",
    "        corr_strength_helper = []\n",
    "        for j in range(len(cat_vars)):\n",
    "            if interp:\n",
    "                if abs(result[n+j][1]['results'][2]) > .25:\n",
    "                    #print(\"Very Strong\")\n",
    "                    corr_strength_helper.append('VS')\n",
    "                elif abs(result[n+j][1]['results'][2]) > .15:\n",
    "                    #print(\"Strong\")\n",
    "                    corr_strength_helper.append('S')\n",
    "                elif abs(result[n+j][1]['results'][2]) > .1:\n",
    "                    #print(\"Moderate\")\n",
    "                    corr_strength_helper.append('M')\n",
    "                elif abs(result[n+j][1]['results'][2]) > .05:\n",
    "                    #print(\"Weak\")\n",
    "                    corr_strength_helper.append('W')\n",
    "                else:\n",
    "                    #print(\"None or Very Weak\")\n",
    "                    corr_strength_helper.append('N')\n",
    "            else:\n",
    "                corr_strength_helper.append(result[n+j][1]['results'][2])\n",
    "\n",
    "        corr_strength_helper = pd.Series(corr_strength_helper,name=cat_vars[i])\n",
    "        corr_strength = pd.concat([corr_strength,corr_strength_helper],axis=1)\n",
    "\n",
    "        n = n + len(cat_vars)\n",
    "\n",
    "    corr_strength.index = cat_vars\n",
    "    \n",
    "    return corr_strength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf32b8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_strength = catCorrMatrix(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c160fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlightS(x,color):\n",
    "    ones = np.where(x > .99, \"color: white;\", None)\n",
    "    s = np.where(x > .2, f\"color: {color};\", None)\n",
    "    vs = np.where(x > .5, \"color: blue;\", None)\n",
    "    for i in range(len(vs)):\n",
    "        if ones[i] == None:\n",
    "            if not s[i] == None:\n",
    "                if vs[i] == None:\n",
    "                    vs[i] = s[i]\n",
    "        else:\n",
    "            vs[i] = ones[i]\n",
    "    return vs\n",
    "\n",
    "def highlightN(x,color):\n",
    "    return np.where((x == \"N\"), f\"color: {color};\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90b6ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(corr_strength.style.apply(highlightS,color=\"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6075af52",
   "metadata": {},
   "source": [
    "From these we can see a lot of strong and very strong correlation between our categorical variables. The main problematic correlations seem to fall into two groups.\n",
    "\n",
    "The first main group is as follows:\n",
    "\n",
    "1. Point of Contact and Area Type\n",
    "2. Point of Contact and Mumbai\n",
    "3. Mumbai and Area Type\n",
    "4. Family and Area Type\n",
    "\n",
    "In addition to these there is a lot of correlation between the different one-hot encoded cities.\n",
    "\n",
    "5. Bangalore and Mumbai\n",
    "6. Bangalore and Chennai\n",
    "7. Bangalore and Hyderabad\n",
    "8. Chennai and Hyderabad\n",
    "9. Chennai and Mumbai\n",
    "10. Hyderabad and Mumbai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7574d888",
   "metadata": {},
   "source": [
    "**First, we can see that Area Type and Point of Contact are collinear, along with Area Type being collinear with Mumbai and Family as well. This seems to indicate that Area Type doesn't present any information not within those other vars, so we can just drop it [IF THIS DOESN'T WORK WE MAY NEED TO SCALE SIZE BY AREA TYPE BEFORE DROPPING IT]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96439a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"Area Type\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bace44",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = [\"Furnishing Status\",\"Point of Contact\",\"Bachelors\",\"Family\",\n",
    "            \"Bangalore\",\"Chennai\",\"Delhi\",\"Hyderabad\",\"Kolkata\",\"Mumbai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc5abc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getCatCorrelation(data)\n",
    "corr_strength = catCorrMatrix(result)\n",
    "display(corr_strength.style.apply(highlightS,color=\"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7a6d4",
   "metadata": {},
   "source": [
    "**Especially when it comes to the cities there seems to be tons of multicollinearity between all of them other than Delhi and Kolkata. Probably drop all other cities other than one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c97e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine bangalore, chennai, hyderabad, and mumbai into a column with \n",
    "#0 meaning the apartment is not in any of the listed cities, \n",
    "#while 1 means that it is in ANY of the listed cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a56e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(data[\"Delhi\"])):\n",
    "    in_one_of_cities = False\n",
    "    for j in [\"Bangalore\",\"Chennai\",\"Hyderabad\",\"Delhi\"]:\n",
    "        if data[j][i] == 1:\n",
    "            in_one_of_cities = True\n",
    "    if in_one_of_cities:\n",
    "        data[\"Delhi\"][i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a3dcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"Bangalore\",axis=1)\n",
    "data = data.drop(\"Chennai\",axis=1)\n",
    "data = data.drop(\"Hyderabad\",axis=1)\n",
    "data = data.drop(\"Kolkata\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a31824",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = [\"Furnishing Status\",\"Point of Contact\",\"Bachelors\",\"Family\",\"Delhi\",\"Mumbai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5962ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getCatCorrelation(data)\n",
    "corr_strength = catCorrMatrix(result)\n",
    "display(corr_strength.style.apply(highlightS,color=\"green\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d307c7",
   "metadata": {},
   "source": [
    "We finally have removed most strong collinearity from our categorical variables. Lets check the durbin watson score again now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737bb19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resids = get_residuals(data,LinearRegression(),False)\n",
    "durbin_watson(resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431e9f6",
   "metadata": {},
   "source": [
    "It seems that we are still a little outside of the range considered normal, as we want at least ~1.5. Looking through these variables we can think that seperating Bachelors and Family may have just contributed to our autocorrelation. Removing one should leave most of the relevant information with the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b054ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_vars = [\"Delhi\",\"Furnishing Status\",\"Family\",\"Mumbai\"]\n",
    "cat_vars = [\"Furnishing Status\",\"Point of Contact\",\"Family\",\"Delhi\",\"Mumbai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1022ae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"Bachelors\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d924c9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getCatCorrelation(data)\n",
    "corr_strength = catCorrMatrix(result)\n",
    "display(corr_strength.style.apply(highlightS,color=\"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47927691",
   "metadata": {},
   "outputs": [],
   "source": [
    "resids = get_residuals(data,LinearRegression(),False)\n",
    "durbin_watson(resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49da9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"Point of Contact\",axis=1)\n",
    "\n",
    "cat_vars = [\"Furnishing Status\",\"Family\",\"Delhi\",\"Mumbai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f34f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getCatCorrelation(data)\n",
    "corr_strength = catCorrMatrix(result)\n",
    "display(corr_strength.style.apply(highlightS,color=\"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d47d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "resids = get_residuals(data,LinearRegression(),False)\n",
    "durbin_watson(resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ee9dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(\"Furnishing Status\",axis=1)\n",
    "\n",
    "cat_vars = [\"Family\",\"Delhi\",\"Mumbai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da5076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getCatCorrelation(data)\n",
    "corr_strength = catCorrMatrix(result)\n",
    "display(corr_strength.style.apply(highlightS,color=\"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6a6500",
   "metadata": {},
   "outputs": [],
   "source": [
    "resids = get_residuals(data,LinearRegression(),False)\n",
    "durbin_watson(resids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1585a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.rename(columns={\"Delhi\":\"InSimilarCities\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee245745",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data.drop(\"Mumbai\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce8fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_vars = [\"Family\",\"InSimilarCities\",\"Mumbai\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea73675a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = getCatCorrelation(data)\n",
    "corr_strength = catCorrMatrix(result)\n",
    "display(corr_strength.style.apply(highlightS,color=\"green\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960f3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "resids = get_residuals(data,LinearRegression(),False)\n",
    "durbin_watson(resids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9d1656",
   "metadata": {},
   "source": [
    "**After all this we still only have 1.3. Seeing as we have removed essentially all autocorrelation from our categorical variables by now we can only attribute this to the continuous variables.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f109c5",
   "metadata": {},
   "source": [
    "# FIX CONTINUOUS AUTOCORRELATION NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8930ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5381a5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3bfe3",
   "metadata": {},
   "source": [
    "### After we've removed most of the autocorrelation from our data we just need to remove extreme outlier points and we will have a useable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ca93d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "col = \"Rent\"\n",
    "sample_mean = np.mean(data[col],axis=0)\n",
    "sample_std_dev = np.std(data[col],axis=0)\n",
    "row_pointer = 0\n",
    "for row in range(len(data)):\n",
    "    safe = True\n",
    "    val = data.iloc[row_pointer][col]\n",
    "    if val <= sample_mean - 2 * sample_std_dev:\n",
    "        safe = False\n",
    "    elif val >= sample_mean + 2 * sample_std_dev:\n",
    "        safe = False\n",
    "\n",
    "    if not safe:\n",
    "        data = data.drop(row,axis=0)\n",
    "        row_pointer = row_pointer - 1\n",
    "\n",
    "    row_pointer = row_pointer + 1\n",
    "data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9411bdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253fdc32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbf3bcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a25b1f36",
   "metadata": {},
   "source": [
    "# Model Usability Experimentation and Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f447d4",
   "metadata": {},
   "source": [
    "### To start model development lets begin by looking at the main types of regression techniques we could potentially use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294df9d9",
   "metadata": {},
   "source": [
    "1. Multiple Linear Regression - By this I mean a regression done with the simple/multiple linear regression model ((((EQUATION -   Y = XB + epsilon)))). Uses OLS or Ordinary Least Squares [SUM:(yHat - y)^2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7784380",
   "metadata": {},
   "source": [
    "2. Neural network regression - This type of model has its advantages, but before testing I think our data sample size is too small from which to learn anything meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b6e740",
   "metadata": {},
   "source": [
    "3. Lasso Regression - Uses Least Absolute Shrinkage and Selection Operator in place of the classic OLS. Good for datasets with highly correlated variables which you're having a hard time seperating."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665b5b35",
   "metadata": {},
   "source": [
    "4. Decision Tree or Random Forest Regression - Using either a single main decision tree or a forest of small trees built from random sampling, either case uses these trees to evaluate a sum as the nodes are evaluated which works in the end as a regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7901e24e",
   "metadata": {},
   "source": [
    "5. KNN (or other clustering) - Cluster the data using a KNN or other clustering algorithm, and evaluate the value of any newly presented point by taking the mean of the nearby points after the new point is plotted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77647190",
   "metadata": {},
   "source": [
    "6. SVM - SVM's are excellent for both classification and regression due to their use of the kernel trick to create arbitrarily complex domains in which to perform the regression without knowing what those domains are. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d704ad",
   "metadata": {},
   "source": [
    "7. Gaussian Process Regression - Models the probability distribution for the domain of all inputs to the function used rather than for any specific inputs. Sounds really nice, and each prediction comes with a uncertainty measure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2c450e",
   "metadata": {},
   "source": [
    "8. Polynomial Regression - Like simple linear regression, but instead of y = x0 + b(x1), the series is increasing polynomials. Therefore, y = x0 + a(x1) + b(x1)^2 + c(x1)^3 + ... n(x1)^n. Seems a little useful, but I have too many variables to use a simple regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b564dc5a",
   "metadata": {},
   "source": [
    "### Now we just need to decide actual models to test based on these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0100992",
   "metadata": {},
   "source": [
    "1. Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5baf07b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(data,reg_type='linear',repeats=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241a8462",
   "metadata": {},
   "source": [
    "2. Neural Network Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa64864d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aecab90d",
   "metadata": {},
   "source": [
    "3. Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3afb9d5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validate(data,reg_type='lasso',repeats=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063070cf",
   "metadata": {},
   "source": [
    "4. Random Forest Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaaaa05",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(data,reg_type='random_forest',repeats=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9181046",
   "metadata": {},
   "source": [
    "5. K-Means Clustering with an SVM Regression per Cluster\n",
    "https://blog.paperspace.com/svr-kmeans-clustering-for-regression/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb70e1f",
   "metadata": {},
   "source": [
    "6. SVM with Linear Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843f22b6",
   "metadata": {},
   "source": [
    "7. SVM with Gaussian Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272294bf",
   "metadata": {},
   "source": [
    "8. SVM with Linear Kernel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
